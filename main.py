# ivf_news_aggregator.py
import sys
import importlib.util
import os
import logging
import logging.handlers
import hashlib
import time
import json
import re
import asyncio
import feedparser
import httpx
import numpy as np
from datetime import datetime, timedelta
from sqlalchemy import create_engine, Column, String, Text, JSON, DateTime
from sqlalchemy.orm import declarative_base, sessionmaker, scoped_session
from sqlalchemy.exc import SQLAlchemyError
from contextlib import contextmanager
from dotenv import load_dotenv
from telegram import Bot, Update
from telegram.error import TelegramError, RetryAfter
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackContext
from http.server import BaseHTTPRequestHandler, HTTPServer
import threading

import tracemalloc
tracemalloc.start(25)


# –ü–∞—Ç—á –¥–ª—è –º–æ–¥—É–ª—è cgi –≤ Python 3.13+
if sys.version_info >= (3, 13):
    cgi_spec = importlib.util.spec_from_loader('cgi', loader=None)
    cgi_module = importlib.util.module_from_spec(cgi_spec)
    sys.modules['cgi'] = cgi_module

    def escape(s, quote=True):
        replacements = {
            '&': '&amp;',
            '<': '&lt;',
            '>': '&gt;',
            '"': '&quot;' if quote else '"',
            "'": '&#x27;' if quote else "'"
        }
        return ''.join(replacements.get(c, c) for c in s)

    cgi_module.escape = escape

from dotenv import load_dotenv

# === –ó–ê–ì–†–£–ó–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø ===
load_dotenv()

# üéØ –û–¢–î–ï–õ–¨–ù–´–ô –õ–û–ì–ì–ï–† –î–õ–Ø –ë–î - –î–û–ë–ê–í–ò–¢–¨ –ò–ú–ï–ù–ù–û –ó–î–ï–°–¨:
db_logger = logging.getLogger('database')
db_logger.setLevel(logging.INFO)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ë–î
db_handler = logging.StreamHandler()
db_handler.setFormatter(logging.Formatter(
    '%(asctime)s - üóÑÔ∏è [DATABASE] - %(levelname)s - %(message)s'
))
db_logger.addHandler(db_handler)
db_logger.propagate = False  # –ù–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ª–æ–≥–µ


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("aggregator.log"),
        logging.handlers.RotatingFileHandler(  
            "aggregator.log", maxBytes=10*1024*1024, backupCount=5
        )
    ]
)
logger = logging.getLogger("IVFNewsAggregator")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
class Config:
    # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã AI: 'local' –∏–ª–∏ 'api'
    AI_MODE = os.getenv("AI_MODE", "local").lower()
    AI_API_URL = os.getenv("AI_API_URL")
    AI_MODEL = os.getenv("AI_MODEL")
    AI_API_KEY = os.getenv("AI_API_KEY")
    AI_TEMPERATURE = float(os.getenv("AI_TEMPERATURE", 0.7))
    AI_MAX_TOKENS = int(os.getenv("AI_MAX_TOKENS", 350))
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://user:password@localhost/dbname")
    @classmethod  
    def get_database_url(cls):
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π URL –¥–ª—è SQLAlchemy"""
        database_url = cls.DATABASE_URL
        if not database_url:
            raise RuntimeError("DATABASE_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–ª—è SQLAlchemy —Å psycopg2
        if database_url.startswith("postgres://"):
            database_url = database_url.replace("postgres://", "postgresql+psycopg2://", 1)
        elif database_url.startswith("postgresql://") and "psycopg2" not in database_url:
            database_url = database_url.replace("postgresql://", "postgresql+psycopg2://", 1)

        return database_url

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Telegram
    TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
    GROUP_CHAT_ID = os.getenv("GROUP_CHAT_ID", "@Futurefamilylab")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL", 1800))  # 30 –º–∏–Ω—É—Ç
    MAX_TEXT_LENGTH = 4096
    EMBEDDING_UPDATE_INTERVAL = 1800  # 30 –º–∏–Ω—É—Ç
    MAX_NEWS_PER_CYCLE = 10  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Ü–∏–∫–ª
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ RSS-—Ñ–∏–¥—ã
    RSS_FEEDS = [
        "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
        "https://news.search.yahoo.com/search;_ylt=Awrhbcnaco5oAwIAl.ZXNyoA;_ylu=Y29sbwNiZjEEcG9zAzEEdnRpZAMEc2VjA3Nj?p=surrogacy&fr=yfp-t&fr2=p%3As%2Cv%3Aw%2Cm%3Anewsdd_bn_m%2Cct%3Abing",
        "https://www.fertilitynetworkuk.org/feed/",
        "https://www.news-medical.net/tag/feed/ivf.aspx",
        "https://www.technologyreview.com/feed/",
        "https://www.sciencedaily.com/rss/health_medicine/fertility.xml",
        "https://www.medicalnewstoday.com/categories/fertility/rss",
        "https://www.straitstimes.com/news/health/rss",
        "https://www.eurekalert.org/news/rss/health.xml",
        "https://www.nature.com/subjects/health-care.rss",
        "https://www.lemonde.fr/sante/rss_full.xml",
        "https://www.spiegel.de/gesundheit/index.rss",
        "https://www.repubblica.it/salute/rss",
        "https://www.japantimes.co.jp/feed/tag/health/",
        "http://www.xinhuanet.com/english/rss/healthrss.xml",
        "https://news.google.com/search?q=%22IVF%7CFertility%7CEmbryo%7CPGT%7CSurrogacy%7CSpermDonation%7CEggDonation%7CFertilityLaw%22&hl=en-US&gl=US&ceid=US%3Aen",  
        "https://news.google.com/rss/search?q=IVF%7CFertility%7CEmbryo%7CPGT%7CSurrogacy%7CSpermDonation%7CEggDonation%7CFertilityLaw&hl=ru-RU&gl=US&ceid=US:ru",
        'https://www.lefigaro.fr/rss/figaro_sante.xml',
        'https://news.google.com/rss/search?q=IVF%7CFertility%7CEmbryo%7CPGT%7CSurrogacy%7CSpermDonation%7CEggDonation%7CFertilityLaw&hl=ru-RU&gl=US&ceid=US:ru',
        'https://www.elmundo.es/rss/salud.xml',
        'https://xml2.corriereobjects.it/rss/salute.xml',
        'https://www.lastampa.it/rss/salute',
        'https://www.scmp.com/rss/4/feed',
        'https://www.abc.es/rss/feeds/abc_Salud.xml',
        'https://elpais.com/salud-y-bienestar/rss/',
        'https://www.faz.net/aktuell/gesundheit/rss.xml',
        'https://www.liberation.fr/sante/rss',
        'https://news.google.com/rss/search?q=IVF+Fertility+Embryo+PGT+Surrogacy+SpermDonation+EggDonation+FertilityLaw&hl=ru&gl=RU&ceid=RU:ru',
        'https://ccrmivf.com/feed',
        'https://doctoreko.ru/news/rss',
        'https://www.sciencedaily.com/rss/health_medicine/fertility.xml',
        'https://www.sciencedaily.com/rss/health_medicine/gynecology.xml', 
        'https://www.sciencedaily.com/rss/health_medicine/pregnancy_and_childbirth.xml', 
        'https://www.nature.com/subjects/embryology.rss',
        'https://www.sciencedaily.com/rss/plants_animals/genetics.xml', 
        'https://www.sciencedaily.com/rss/health_medicine/genes.xml',
        'https://www.sciencedaily.com/rss/health_medicine/stem_cells.xml',
        'https://www.sciencedaily.com/rss/plants_animals/developmental_biology.xml', 
        'https://www.sciencedaily.com/rss/health_medicine/epigenetics.xml',
        'https://www.sciencedaily.com/rss/health_medicine/birth_defects.xml',
        'https://www.sciencedaily.com/rss/health_medicine/gene_therapy.xml',
        'https://www.sciencedaily.com/rss/plants_animals/cloning.xml',
        'https://elementy.ru/rss/news',
        'http://rss.sciam.com/basic-science', 
        'https://www.sciencedaily.com/rss/all.xml', 
        'https://www.sciencedaily.com/rss/top/health.xml',  
        'https://www.sciencedaily.com/rss/top/science.xml', 
        'https://www.theguardian.com/science/rss',
        'https://www.technologyreview.com/feed/',
        'https://www.sciencedaily.com/rss/science_society/bioethics.xml',
        'https://www.theguardian.com/rss',

    ]
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    KEYWORDS = [
        'ivf', 'fertility', '—ç–∫–æ', '—Ñ–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å', 'embryo', '—ç–º–±—Ä–∏–æ–Ω', 'egg donation',
        'in vitro', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–ø–ª–æ–¥–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ', '—Ä–µ–ø—Ä–æ–¥—É–∫—Ü–∏—è', 'surrogacy',
        'infertility', '–±–µ—Å–ø–ª–æ–¥–∏–µ', 'egg freezing', '–∫—Ä–∏–æ–∫–æ–Ω—Å–µ—Ä–≤–∞—Ü–∏—è',
        'sperm donation', '–¥–æ–Ω–æ—Ä—Å—Ç–≤–æ —Å–ø–µ—Ä–º—ã', 'embryo transfer', '–ø–µ—Ä–µ–Ω–æ—Å —ç–º–±—Ä–∏–æ–Ω–æ–≤',
        'fertility treatment', '–ª–µ—á–µ–Ω–∏–µ –±–µ—Å–ø–ª–æ–¥–∏—è', 'assisted reproduction',
        '–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–ø–ª–æ–¥', '–∑–∞—á–∞—Ç–∏–µ'
    ]

    # –ú–æ–¥–µ–ª–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ò–ò
    LOCAL_AI_MODELS = {
        "embedding": "sentence-transformers/all-MiniLM-L6-v2",
        "summarization": "IlyaGusev/rut5_base_sum_gazeta",
        "translation": "Helsinki-NLP/opus-mt-en-ru"
    }
    
    # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≥–ª–æ—Å—Å–∞—Ä–∏–π (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–ø—è—Ç–∞—è –ø–æ—Å–ª–µ "–≥–µ—Å—Ç–∞—Ü–∏—è")
    MEDICAL_GLOSSARY = {
        "ivf": "–≠–ö–û",
        "in vitro fertilization": "—ç–∫—Å—Ç—Ä–∞–∫–æ—Ä–ø–æ—Ä–∞–ª—å–Ω–æ–µ –æ–ø–ª–æ–¥–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ",
        "embryo": "—ç–º–±—Ä–∏–æ–Ω",
        "implantation": "–∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏—è",
        "fertility": "—Ñ–µ—Ä—Ç–∏–ª—å–Ω–æ—Å—Ç—å",
        "blastocyst": "–±–ª–∞—Å—Ç–æ—Ü–∏—Å—Ç–∞",
        "ovarian stimulation": "—Å—Ç–∏–º—É–ª—è—Ü–∏—è —è–∏—á–Ω–∏–∫–æ–≤",
        "sperm": "—Å–ø–µ—Ä–º–∞—Ç–æ–∑–æ–∏–¥",
        "oocyte": "–æ–æ—Ü–∏—Ç",
        "zygote": "–∑–∏–≥–æ—Ç–∞",
        "gestation": "–≥–µ—Å—Ç–∞—Ü–∏—è",  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–ø—è—Ç–∞—è
        "PGT": "–ü–ì–¢ (–ø—Ä–µ–∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)",
        "ICSI": "–ò–ö–°–ò (–∏–Ω—Ç—Ä–∞—Ü–∏—Ç–æ–ø–ª–∞–∑–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—ä–µ–∫—Ü–∏—è —Å–ø–µ—Ä–º–∞—Ç–æ–∑–æ–∏–¥–∞)",
        "ovulation induction": "–∏–Ω–¥—É–∫—Ü–∏—è –æ–≤—É–ª—è—Ü–∏–∏",
        "endometrium": "—ç–Ω–¥–æ–º–µ—Ç—Ä–∏–π",
        "follicle": "—Ñ–æ–ª–ª–∏–∫—É–ª",
        "gamete": "–≥–∞–º–µ—Ç–∞",
        "zygote intrafallopian transfer": "–∑–∏–≥–æ—Ç–Ω—ã–π –≤–Ω—É—Ç—Ä–∏—Ñ–∞–ª–ª–æ–ø–∏–µ–≤—ã–π –ø–µ—Ä–µ–Ω–æ—Å",
        "surrogacy": "—Å—É—Ä—Ä–æ–≥–∞—Ç–Ω–æ–µ –º–∞—Ç–µ—Ä–∏–Ω—Å—Ç–≤–æ"
    }

# –ú–æ–¥–µ–ª—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
Base = declarative_base()

class NewsItem(Base):
    __tablename__ = 'news'

    id = Column(String(32), primary_key=True)
    url = Column(String(1024), unique=True)
    title = Column(String(512))
    original_text = Column(Text)
    summary = Column(Text)
    image_url = Column(String(1024), nullable=True)
    embedding = Column(JSON)              # —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    summary_embedding = Column(JSON)      # —ç–º–±–µ–¥–¥–∏–Ω–≥ summary
    published_at = Column(DateTime, default=datetime.utcnow)

    def __repr__(self):
        return f"<NewsItem({self.title})>"
    
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
async def is_valid_article(url: str, client: httpx.AsyncClient) -> bool:
    try:
        response = await client.head(url, follow_redirects=True, timeout=10.0)
        return response.status_code == 200 and 'html' in response.headers.get('content-type', '')
    except Exception as e:
        logger.warning(f"Invalid article URL: {url} ({e})")
        return False

async def fetch_full_text(url: str, client: httpx.AsyncClient) -> str:
    try:
        response = await client.get(url, timeout=15.0)
        response.raise_for_status()
        paragraphs = re.findall(r'<p[^>]*>(.*?)</p>', response.text, re.DOTALL)
        text = '\n'.join(re.sub('<[^<]+?>', '', p) for p in paragraphs)
        return text[:Config.MAX_TEXT_LENGTH].strip()
    except Exception as e:
        logger.warning(f"Failed to extract full text from {url}: {e}")
        return ''

def build_telegram_message(news_item, ai_summary):
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    cleaned_summary = re.sub(r'(#{1,6}\s*\**|\**\s*$)', '', ai_summary)
    cleaned_summary = re.sub(r'[#*_]{2,}', '', cleaned_summary)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –∫ –∫–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
    cleaned_summary = cleaned_summary.replace("–ü–æ—á–µ–º—É", "üî¨ <b>–ü–æ—á–µ–º—É")
    cleaned_summary = cleaned_summary.replace("–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ", "üìö –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ")
    cleaned_summary = cleaned_summary.replace("{link_placeholder}", "")
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É - —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–æ "–ò—Å—Ç–æ—á–Ω–∏–∫" –±—É–¥–µ—Ç –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º
    source_link = f'<a href="{news_item.url}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'    
    message = f"{cleaned_summary}\n\nüìñ {source_link}"

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –ø—Ä–∏ –æ–±—Ä–µ–∑–∫–µ
    max_length = Config.MAX_TEXT_LENGTH
    suffix_length = len(suffix) + 4  # +4 –¥–ª—è " ..."
    available_length = max_length - suffix_length
    
    # –û–±—Ä–µ–∑–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π –¥–ª–∏–Ω—ã
    if len(cleaned_summary) > available_length:
    # –ò—â–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–µ—Å—Ç–æ –¥–ª—è –æ–±—Ä—ã–≤–∞
        cutoff_point = cleaned_summary[:available_length].rfind('. ')
        if cutoff_point == -1:
            cutoff_point = cleaned_summary[:available_length].rfind(' ')
        
        if cutoff_point > 0:
            main_text = cleaned_summary[:cutoff_point] + '...'
        else:
            main_text = cleaned_summary[:available_length-3] + '...'
    else:
        main_text = cleaned_summary
    
    return main_text + suffix

# –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
class Database:
    def __init__(self):
        self.engine = None
        self.session_factory = None
        self.Session = None

    def init_db(self):
        try:
            database_url = Config.get_database_url()
            self.engine = create_engine(
                database_url,
                pool_size=10,
                max_overflow=5,
                pool_recycle=3600,
                pool_pre_ping=True,
                connect_args={
                    "sslmode": "require",
                    "connect_timeout": 10,
                    "application_name": "ivf_news_bot"
                }
            )
            Base.metadata.create_all(self.engine)
            self.session_factory = sessionmaker(
                bind=self.engine,
                autocommit=False,
                autoflush=False,
                expire_on_commit=False
            )
            self.Session = scoped_session(self.session_factory)
            db_logger.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            self.test_connection()
        except Exception as e:
            db_logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
            raise

    def test_connection(self):
        try:
            with self.session_scope() as s:
                s.execute("SELECT 1").fetchone()
            db_logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            db_logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            raise

    @contextmanager
    def session_scope(self):
        s = self.Session()
        try:
            yield s
            s.commit()
        except:
            s.rollback()
            raise
        finally:
            s.close()

    def save_news(self, news_item):
        with self.session_scope() as s:
            s.merge(news_item)

    def is_url_processed(self, url):
        url_hash = hashlib.md5(url.encode()).hexdigest()
        with self.session_scope() as s:
            return s.get(NewsItem, url_hash) is not None

    def is_summary_duplicate(self, summary_text):
        with self.session_scope() as s:
            return s.query(NewsItem).filter(NewsItem.summary == summary_text).first() is not None

    def get_all_embeddings(self, max_age_days=30):
        cutoff = datetime.utcnow() - timedelta(days=max_age_days)
        with self.session_scope() as s:
            rows = s.query(NewsItem.id, NewsItem.embedding)\
                    .filter(NewsItem.published_at >= cutoff)\
                    .all()
            result = []
            for rid, emb in rows:
                if emb:
                    try:
                        result.append((rid, np.array(emb)))
                    except Exception:
                        pass
            return result

# –ö–ª–∞—Å—Å –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
class AIProcessor:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π HTTP-–∫–ª–∏–µ–Ω—Ç
            cls._instance.http_client = httpx.AsyncClient(timeout=60.0)
            cls._instance.init_ai()
        return cls._instance
    
    def init_ai(self):
        self.mode = Config.AI_MODE
        self.model_initialized = False
        
        if self.mode == "local":
            self.load_local_models()
        elif self.mode == "api":
            self.model_initialized = True
            logger.info("API mode initialized")
        else:
            raise ValueError(f"Unsupported AI mode: {self.mode}")
    
    def load_local_models(self):
        try:
            import torch
            from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
            from sentence_transformers import SentenceTransformer
            
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"Loading LOCAL models on {self.device.upper()}")
            
            self.embedding_model = SentenceTransformer(
                Config.LOCAL_AI_MODELS["embedding"], 
                device=self.device
            )
            
            self.sum_tokenizer = AutoTokenizer.from_pretrained(
                Config.LOCAL_AI_MODELS["summarization"]
            )
            self.sum_model = AutoModelForSeq2SeqLM.from_pretrained(
                Config.LOCAL_AI_MODELS["summarization"]
            ).to(self.device)
            
            self.translator = pipeline(
                "translation", 
                model=Config.LOCAL_AI_MODELS["translation"],
                device=0 if self.device == "cuda" else -1
            )
            
            logger.info("LOCAL AI models loaded")
            self.model_initialized = True
        except Exception as e:
            logger.error(f"Error loading LOCAL AI models: {str(e)}")
            self.model_initialized = False
    
    def generate_embedding(self, text):
        if self.mode == "api":
            return np.zeros(384)
        
        if not hasattr(self, 'embedding_model'):
            try:
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer(
                    Config.LOCAL_AI_MODELS["embedding"]
                )
            except Exception as e:
                logger.error(f"Failed to load embedding model: {str(e)}")
                return np.zeros(384)
            
        return self.embedding_model.encode([text[:Config.MAX_TEXT_LENGTH]])[0]
    
    def is_duplicate(self, embedding, db_embeddings, threshold=0.85):
        if self.mode == "api" or not db_embeddings or not self.model_initialized:
            return False, None
        
        embedding = np.array(embedding)
        max_similarity = 0
        max_item_id = None
        
        for item_id, db_emb in db_embeddings:
            norm = np.linalg.norm(embedding) * np.linalg.norm(db_emb)
            if norm == 0:
                continue
            similarity = np.dot(embedding, db_emb) / norm
            if similarity > max_similarity:
                max_similarity = similarity
                max_item_id = item_id
        
        return (max_similarity > threshold, max_item_id)
    
    def apply_medical_glossary(self, text):
        for eng, ru in Config.MEDICAL_GLOSSARY.items():
            text = re.sub(rf'\b{eng}\b', f'<b>{ru}</b>', text, flags=re.IGNORECASE)
        return text
    
    async def process_content(self, text):
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥–ª–æ—Å—Å–∞—Ä–∏–π –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        text = self.apply_medical_glossary(text)
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞
        text = re.sub(r'(–ü–æ—á–µ–º—É\s.+?:)', r'üî¨ <b>\1</b>', text)
        text = re.sub(r'(–í–∞–∂–Ω–æ—Å—Ç—å\s.+?:)', r'‚≠ê <b>\1</b>', text)
        text = re.sub(r'(–ö–∞–∫\s.+?:)', r'‚öôÔ∏è <b>\1</b>', text)
        
        if not self.model_initialized:
            return text[:Config.MAX_TEXT_LENGTH]
        
        if self.mode == "api":
            return await self.process_with_api(text)
        
        # –õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        return text[:Config.MAX_TEXT_LENGTH]
    
    async def process_with_api(self, text):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π AI API"""
        if not Config.AI_API_KEY:
            logger.error("AI_API_KEY is not set! Using fallback")
            return text[:200]
        
        for attempt in range(3):
            try:
                headers = {
                    "Authorization": f"Bearer {Config.AI_API_KEY}",
                    "Content-Type": "application/json"
                }
                
                # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è OpenRouter
                prompt = (
                    "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ –ø–æ —Ä–µ–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω–µ. "
                    "–°–æ–∑–¥–∞–π –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è Telegram —Ç–µ–∫—Å—Ç –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º:\n\n"
                
                    "üî• <b>–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Telegram:</b>\n"
                    "1. –ó–∞–≥–æ–ª–æ–≤–æ–∫: –æ–¥–∏–Ω –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –≤ –Ω–∞—á–∞–ª–µ (–±–µ–∑ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –¥—É–±–ª—è)\n"
                    "2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π\n"
                    "3. –†–∞–∑–º–µ—Ç–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–π <b>–∂–∏—Ä–Ω—ã–π</b> –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ <i>–∫—É—Ä—Å–∏–≤</i> –¥–ª—è –≤–∞–∂–Ω—ã—Ö –∞–∫—Ü–µ–Ω—Ç–æ–≤\n"
                    "4. –≠–º–æ–¥–∑–∏: –¥–æ–±–∞–≤–ª—è–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –±–ª–æ–∫–æ–≤\n"
                    "5. –ù–ò–ö–û–ì–î–ê –Ω–µ –≤–∫–ª—é—á–∞–π —Å—Å—ã–ª–∫–∏ –∏–ª–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è '–∏—Å—Ç–æ—á–Ω–∏–∫' –≤ —Ç–µ–∫—Å—Ç–µ!\n\n"
                    "6. –î–ª–∏–Ω–∞: –¥–æ 600 —Å–ª–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ –º–æ–±–∏–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ\n\n"
                
                    "üî¨ <b>–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:</b>\n"
                    "1. –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü: —Å—É—Ç—å –æ—Ç–∫—Ä—ã—Ç–∏—è –∏ –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ä–µ–ø—Ä–æ–¥—É–∫—Ç–æ–ª–æ–≥–∏–∏\n"
                    "2. –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å: –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∏ –Ω–∞—É—á–Ω–æ–π –Ω–æ–≤–∏–∑–Ω—ã\n"
                    "3. –ö–æ–Ω—Ü–æ–≤–∫–∞: –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ—Ç–∫—Ä—ã—Ç–∏—è\n"
                    "4. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∏:\n"
                    "   - –ê–≤—Ç–æ—Ä–æ–≤ –∏ –∂—É—Ä–Ω–∞–ª –ø—É–±–ª–∏–∫–∞—Ü–∏–∏\n"
                    "   - –î–∞—Ç—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
                    "   - –ö–ª–∏–Ω–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–ª—è –≠–ö–û\n\n"
                
                    "‚ö° <b>–°—Ç–∏–ª–∏—Å—Ç–∏–∫–∞:</b>\n"
                    "1. –ù–∞—É—á–Ω–æ-–ø–æ–ø—É–ª—è—Ä–Ω—ã–π —Å—Ç–∏–ª—å —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –Ω–∞—É—á–Ω–æ–π —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏\n"
                    "2. –ò—Å–ø–æ–ª—å–∑—É–π –∞–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: \"–£—á–µ–Ω—ã–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏\" –≤–º–µ—Å—Ç–æ \"–ë—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\"\n"
                    "3. –î–æ–±–∞–≤—å 1-2 —Ü–∏—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–¥–∞–∂–µ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)\n"
                    "4. –°–æ–±–ª—é–¥–∞–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤:\n"
                    "   - –≠–º–±—Ä–∏–æ–Ω (–¥–æ 8 –Ω–µ–¥–µ–ª—å) ‚Üí –ü–ª–æ–¥ (–ø–æ—Å–ª–µ 8 –Ω–µ–¥–µ–ª—å)\n"
                    "   - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –ü–ì–¢, –ò–ö–°–ò, –∫—Ä–∏–æ–∫–æ–Ω—Å–µ—Ä–≤–∞—Ü–∏—è –∏ —Ç.–¥.\n\n"
                
                    "üö´ <b>–ó–∞–ø—Ä–µ—â–µ–Ω–æ:</b>\n"
                    "- –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–ª–∏ —Å—Å—ã–ª–æ–∫\n"
                    "- –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –¥—É–±–ª–∏\n"
                    "- –ú–∞—Ä–∫–¥–∞—É–Ω-—Ä–∞–∑–º–µ—Ç–∫–∞ (##, **)\n"
                    "- –î–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (>25 —Å–ª–æ–≤)\n"
                    "- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π\n\n"
                
                    "üåü <b>–ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä:</b>\n"
                    "–ü—Ä–∏–º–µ—Ä 1 (–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ):\n"
                    "<b>–ù–æ–≤—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª —Å—Ç–∏–º—É–ª—è—Ü–∏–∏ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≠–ö–û –Ω–∞ 30%</b>\n\n"
                    "üî¨ –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑ –ö–∞—Ä–æ–ª–∏–Ω—Å–∫–æ–≥–æ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Å—Ç–∏–º—É–ª—è—Ü–∏–∏ —è–∏—á–Ω–∏–∫–æ–≤. "
                    "–ú–µ—Ç–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ —Å–∏–Ω–¥—Ä–æ–º–∞ –≥–∏–ø–µ—Ä—Å—Ç–∏–º—É–ª—è—Ü–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\n\n"
                    "üß™ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞...\n\n"
                    "üí° –î–ª—è –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç...\n\n"
                    "üë®‚Äç‚öïÔ∏è –ü—Ä–æ—Ñ–µ—Å—Å–æ—Ä –ê–Ω–¥–µ—Ä—Å—Å–æ–Ω: \"–≠—Ç–æ —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–¥—Ö–æ–¥–∞—Ö –∫...\"\n\n"
                    "üìö Journal of Assisted Reproduction, 10 –æ–∫—Ç—è–±—Ä—è 2025\n\n"
                    
                    "–ü—Ä–∏–º–µ—Ä 2 (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ä—ã–≤):\n"
                    "<b>–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å–ø–µ—Ö –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏ —ç–º–±—Ä–∏–æ–Ω–∞</b>\n\n"
                    "ü§ñ –ê–ª–≥–æ—Ä–∏—Ç–º DeepIVF, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –≤ MIT, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–∫–∏–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–º–±—Ä–∏–æ–Ω–æ–≤ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é 92%...\n\n"
                    "‚öôÔ∏è –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞...\n\n"
                    "üè• –í–Ω–µ–¥—Ä–µ–Ω–∏–µ –≤ –∫–ª–∏–Ω–∏–∫–∞—Ö –æ–∂–∏–¥–∞–µ—Ç—Å—è...\n\n"
                    
                    "–ü—Ä–∏–º–µ—Ä 3 (—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –∞—Å–ø–µ–∫—Ç):\n"
                    "<b>–ü—Å–∏—Ö–æ–ª–æ–≥–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π –ø–æ—Å–ª–µ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–Ω—Å—Ç–≤–∞</b>\n\n"
                    "üß† –ù–æ–≤–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ –ø–æ–º–æ–≥–∞–µ—Ç —Å–µ–º—å—è–º...\n\n"
                    "‚ù§Ô∏è –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è...\n\n"
                    "üë∂ –î–æ–∫—Ç–æ—Ä –ü–µ—Ç—Ä–æ–≤–∞: \"–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–≤—è–∑—å —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è...\"\n\n"
                    
                    f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text[:3000]}"
                                       
                )

                payload = {
                    "model": Config.AI_MODEL,
                    "messages": [{
                        "role": "user",
                        "content": prompt
                    }],
                    "temperature": Config.AI_TEMPERATURE,
                    "max_tokens": Config.AI_MAX_TOKENS
                }
                
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º httpx
                response = await self.http_client.post(
                    Config.AI_API_URL,
                    headers=headers,
                    json=payload,
                    timeout=60
                )
                response.raise_for_status()
                result = response.json()
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ OpenRouter
                return result['choices'][0]['message']['content'].strip()
            except Exception as e:
                logger.error(f"AI API error (attempt {attempt+1}/3): {str(e)}")
                if attempt < 2:
                    await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                else:
                    return text[:200]

    def translate_to_russian(self, text):
        if not self.model_initialized:
            return text
            
        text = self.apply_medical_glossary(text[:Config.MAX_TEXT_LENGTH])
        try:
            return self.translator(text)[0]['translation_text']
        except Exception as e:
            logger.error(f"Translation error: {str(e)}")
            return text
    
    def detect_language(self, text):
        return 'ru' if any('\u0400' <= char <= '\u04FF' for char in text) else 'en'

# –ö–ª–∞—Å—Å –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –≤ Telegram
class TelegramNotifier:
    def __init__(self, ai_processor=None):
        self.token = Config.TELEGRAM_BOT_TOKEN
        self.group_chat_id = Config.GROUP_CHAT_ID
        self.ai = ai_processor
        self.semaphore = asyncio.Semaphore(5)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –æ—Ç–ø—Ä–∞–≤–æ–∫
        
        logger.info(f"Initializing Telegram bot with token: {self.token[:10]}...")
        logger.info(f"Target group: {self.group_chat_id}")
        
        try:
            self.bot = Bot(token=self.token)
            logger.info("Telegram bot initialized with default timeouts")
        except Exception as e:
            logger.error(f"Failed to initialize Telegram bot: {str(e)}")
            self.bot = None
    
    async def send_message(self, text: str):
        if not self.bot:
            logger.warning("Telegram bot not available")
            return
        
        async with self.semaphore:
            for attempt in range(3):
                try:
                    await self.bot.send_message(
                        chat_id=self.group_chat_id,
                        text=text,
                        parse_mode="HTML"
                    )
                    logger.info("Telegram message sent")
                    return
                except RetryAfter as e:
                    wait_time = e.retry_after
                    logger.warning(f"Rate limited, retrying in {wait_time} seconds")
                    await asyncio.sleep(wait_time)
                except TelegramError as e:
                    logger.error(f"Telegram send error (attempt {attempt+1}/3): {e}")
                    if attempt < 2:
                        await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    else:
                        logger.error("Failed to send Telegram message after 3 attempts")
                        return
    
    async def send_news(self, news_item):

        if not news_item.summary:
            logger.error("No summary for news item, skipping")
            return
        
            message = build_telegram_message(news_item, news_item.summary)
    
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        #logger.info(f"Prepared message: {message[:100]}...")
        #logger.info(f"Image URL: {getattr(news_item, 'image_url', 'N/A')}")
    
        message = build_telegram_message(news_item, news_item.summary)
        #image_url = getattr(news_item, 'image_url', None)
        
        try:
            #if image_url:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                #async with httpx.AsyncClient() as client:
                    #response = await client.head(image_url)
                    #if response.status_code == 200 and 'image' in response.headers.get('content-type', ''):
                        #await self.bot.send_photo(
                            #chat_id=self.group_chat_id,
                            #photo=image_url,
                            #caption=message,
                            #parse_mode="HTML"
                        #)
                        #return)
                
            # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            await self.bot.send_message(
                chat_id=self.group_chat_id,
                text=message,
                parse_mode="HTML"
            )
        
        except Exception as e:
            logger.error(f"Failed to send news: {str(e)}")
            # –§–æ–ª–±—ç–∫ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            await self.bot.send_message(
                chat_id=self.group_chat_id,
                text=message,
                parse_mode="HTML"
            )

    async def handle_command(self, update: Update, context: CallbackContext):
        if not self.ai:
            await context.bot.send_message(
                chat_id=update.effective_chat.id,
                text="‚ùå AI processor not available"
            )
            return
        
        test_text = "In vitro fertilisation (IVF) is a process of fertilisation where an egg is combined with sperm in vitro."
        result = await self.ai.process_content(test_text)
        await context.bot.send_message(
            chat_id=update.effective_chat.id,
            text=f"AI Test Result:\n{result}"
        )
    
    async def start_polling(self):
        if not self.token:
            logger.warning("Telegram token not set")
            return
            
        try:
            application = ApplicationBuilder().token(self.token).build()
            application.add_handler(CommandHandler("test_ai", self.handle_command))
            
            logger.info("Starting Telegram polling")
            await application.run_polling()
        except Exception as e:
            logger.error(f"Polling error: {str(e)}")

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞
class NewsAggregator:
    def __init__(self):
        self.db = Database()
        self.db.init_db()
        self.ai = AIProcessor()
        self.notifier = TelegramNotifier(self.ai)
        self.db_embeddings = []
        self.last_embeddings_update = 0
        self.news_counter = 0  # –°—á–µ—Ç—á–∏–∫ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π HTTP-–∫–ª–∏–µ–Ω—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ RSS
        self.http_client = httpx.AsyncClient(
            headers={
                "User-Agent": "Mozilla/5.0 (compatible; IVF-News-Aggregator/1.0)",
                "Accept": "application/rss+xml, text/xml;q=0.9"
            },
            timeout=30.0,
            follow_redirects=True
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        self.update_embeddings()
        
        # –ó–∞–ø—É—Å–∫ —Å–ª—É—à–∞—Ç–µ–ª—è Telegram –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        asyncio.create_task(self.notifier.start_polling())

    def update_embeddings(self):
        self.db_embeddings = self.db.get_all_embeddings()
        self.last_embeddings_update = time.time()  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ last_embeddings_update
        logger.info(f"Embeddings cache updated, {len(self.db_embeddings)} items")

    async def safe_parse_feed(self, feed_url):
        try:
            response = await self.http_client.get(feed_url)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å RSS
            content_type = response.headers.get('Content-Type', '').lower()
            if 'xml' not in content_type and 'rss' not in content_type:
                logger.warning(f"Invalid content type for {feed_url}: {content_type}")
                return None
            
            return feedparser.parse(response.content)
        except Exception as e:
            logger.error(f"Feed parse error [{feed_url}]: {str(e)}")
            return None

    def is_ivf_related(self, text: str) -> bool:
        if not text:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_words = ['—ç–∫–æ–Ω–æ–º–∏–∫–∞', '–ø—É—Ç–∏–Ω', '—Å—Ç–∞–≤–∫–∞', '–±–∞–Ω–∫', 
                 '–º–∏–Ω—ç–∫–æ–Ω–æ–º—Ä–∞–∑–≤–∏—Ç–∏—è', '—Å–æ–≤—Ñ–µ–¥', '—Ü–± —Ä—Ñ']
        for word in stop_words:
            if word in text.lower():
                return False
    
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        text_lower = text.lower()
        for keyword in Config.KEYWORDS:
            if keyword.lower() in text_lower:
                return True
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è
        for term in Config.MEDICAL_GLOSSARY.values():
            if term.lower() in text_lower:
                return True
            
        return False

    async def process_feed(self, feed_url):
        parsed = await self.safe_parse_feed(feed_url)
        if not parsed or not parsed.entries:
            return []

        new_items = []
        for entry in parsed.entries:
            try:
                if not hasattr(entry, 'link') or not entry.link:
                    continue

                # –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π URL?
                if self.db.is_url_processed(entry.link):
                    continue

                # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                title = entry.title[:512] if hasattr(entry, 'title') else ""
                description = entry.get('description', '')[:5000]
                preview_text = f"{title} {description}"
                if not self.is_ivf_related(preview_text):
                    continue

                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏
                if not await is_valid_article(entry.link, self.http_client):
                    continue

                # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                full_text = await fetch_full_text(entry.link, self.http_client)
                title = entry.title[:512] if hasattr(entry, 'title') else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                combined_text = f"{title}\n\n{description}\n\n{full_text}"[:Config.MAX_TEXT_LENGTH]

                # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
                if not self.is_ivf_related(combined_text):
                    logger.info(f"Skipped non-IVF article: {title[:50]}...")
                    continue

                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                if time.time() - self.last_embeddings_update > Config.EMBEDDING_UPDATE_INTERVAL:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, self.update_embeddings)

                # –≠–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                embedding = self.ai.generate_embedding(combined_text)

                # –î—É–±–ª–∏–∫–∞—Ç –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥—É –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞?
                is_dup, dup_id = self.ai.is_duplicate(embedding, self.db_embeddings)
                if is_dup:
                    logger.info(f"Duplicate by full text: {title[:20]}... similar to {dup_id}")
                    continue

                # –†–µ—Ä–∞–π—Ç (summary) –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ summary
                summary = await self.ai.process_content(combined_text) if self.ai.model_initialized else combined_text[:200]
                summary_embedding = self.ai.generate_embedding(summary)

                # –î—É–±–ª–∏–∫–∞—Ç –ø–æ summary (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)?
                if self.db.is_summary_duplicate(summary):
                    logger.info(f"Duplicate by summary: {title[:20]}...")
                    continue

                # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                published_at = datetime.utcnow()
                if hasattr(entry, 'published_parsed'):
                    try:
                        published_at = datetime(*entry.published_parsed[:6])
                    except (TypeError, ValueError):
                        pass

                # –ö–∞—Ä—Ç–∏–Ω–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                image_url = None
                if hasattr(entry, 'media_content') and entry.media_content:
                    image_url = entry.media_content[0].get('url')
                    if image_url:
                        image_url = image_url[:1024]

                # ID –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                url_hash = hashlib.md5(entry.link.encode()).hexdigest()
                news_item = NewsItem(
                    id=url_hash,
                    url=entry.link[:1024],
                    title=title[:512],
                    original_text=combined_text,
                    summary=summary,
                    embedding=embedding.tolist(),
                    summary_embedding=summary_embedding.tolist(),
                    published_at=published_at,
                    image_url=image_url
                )

                self.db.save_news(news_item)
                new_items.append(news_item)

                # –û–±–Ω–æ–≤–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                self.db_embeddings.append((url_hash, np.array(embedding)))

                self.news_counter += 1
                await self.notifier.send_news(news_item)

                if self.news_counter >= Config.MAX_NEWS_PER_CYCLE:
                    logger.info(f"Reached max news limit ({Config.MAX_NEWS_PER_CYCLE}) for this cycle")
                    break

            except Exception as e:
                logger.error(f"Entry processing error: {str(e)}")

        return new_items



    async def run(self):
        logger.info(f"Starting IVF News Aggregator [{Config.AI_MODE} AI mode]")
        logger.info(f"Monitoring {len(Config.RSS_FEEDS)} feeds")
        
        if not self.ai.model_initialized:
            logger.critical("AI initialization failed!")
            await self.notifier.send_message("‚ùå IVF News Aggregator FAILED to start: AI not initialized!")
            return
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Telegram
        try:
            await self.notifier.send_message("ü§ñ IVF News Aggregator started successfully!")
        except Exception as e:
            logger.error(f"Failed to send startup message to Telegram: {str(e)}")
        
        while True:
            start_time = time.time()
            logger.info(f"Processing cycle started")
            self.news_counter = 0  # –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
            
            total_new = 0
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∏–¥–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏
            for i, feed_url in enumerate(Config.RSS_FEEDS):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
                if self.news_counter >= Config.MAX_NEWS_PER_CYCLE:
                    logger.info(f"Max news per cycle reached ({Config.MAX_NEWS_PER_CYCLE}), skipping remaining feeds")
                    break
                    
                try:
                    new_items = await self.process_feed(feed_url)
                    total_new += len(new_items)
                    logger.info(f"Feed processed: {feed_url} - {len(new_items)} new")
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ñ–∏–¥–∞–º–∏
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"Feed processing error: {str(e)}")
            
            cycle_time = time.time() - start_time
            logger.info(f"Cycle completed: {total_new} new items, {cycle_time:.2f} sec")
            
            sleep_time = max(Config.CHECK_INTERVAL - cycle_time, 60)
            logger.info(f"Sleeping for {sleep_time} seconds")
            await asyncio.sleep(sleep_time)


class HealthCheckHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.handle_request()
    
    def do_HEAD(self):
        self.handle_request()
    
    def handle_request(self):
        if self.path == '/' or self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'text/plain')
            self.end_headers()
            if self.command == 'GET':
                self.wfile.write(b'OK')
        else:
            self.send_response(404)
            self.end_headers()

def run_http_server(port):
    server_address = ('', port)
    httpd = HTTPServer(server_address, HealthCheckHandler)
    logger.info(f"Starting health check server on port {port}")
    httpd.serve_forever()

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
import atexit
atexit.register(tracemalloc.stop)

async def main():
    """–ì–ª–∞–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞"""
    aggregator = None
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ (—É–±—Ä–∞–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ)
        aggregator = NewsAggregator()
        logger.info("–ù–æ–≤–æ—Å—Ç–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ (—Ç–µ–ø–µ—Ä—å –í–ù–£–¢–†–ò try –±–ª–æ–∫–∞)
        await aggregator.run()
        
    except asyncio.CancelledError:
        logger.info("–ó–∞–¥–∞—á–∞ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
        
    except Exception as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        logger.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é DATABASE_URL –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Supabase")
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
        if aggregator and hasattr(aggregator, 'notifier'):
            try:
                await aggregator.notifier.send_message(f"‚ùå IVF News Aggregator CRASHED: {str(e)}")
            except Exception as notify_error:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ: {notify_error}")
        
        raise
        
    finally:
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
        if aggregator:
            try:
                if hasattr(aggregator, 'http_client'):
                    await aggregator.http_client.aclose()
                if hasattr(aggregator, 'ai') and hasattr(aggregator.ai, 'http_client'):
                    await aggregator.ai.http_client.aclose()
                logger.info("HTTP-–∫–ª–∏–µ–Ω—Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã—Ç—ã")
            except Exception as cleanup_error:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤: {cleanup_error}")
# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ HTTP-—Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è Render.com
    port = int(os.getenv("PORT", 8080))
    http_thread = threading.Thread(target=run_http_server, args=(port,), daemon=True)
    http_thread.start()
    
    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Aggregator stopped manually")
    except Exception as e:
        logger.critical(f"Unhandled exception: {str(e)}")
